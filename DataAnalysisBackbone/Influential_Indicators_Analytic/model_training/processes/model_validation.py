# -*- coding: utf-8 -*-
"""model_validation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lX18nmbCtuXUoiebHTQhnQhz-rEBHJ21
"""

import duckdb
import os
import glob
import pandas as pd
import numpy as np
import pickle
import json
import sklearn.metrics as metrics




"""## Select Model to Explain

We decided that it would be good that the user can select which model (of the generated experimenting) he will be interested in validate. 

Once the user selects the model, metrics and feature importance is generated and saved in metadata
"""


"""### Loading model"""


def select_model_to_validate():
    dirname = os.path.dirname(__file__)
    models_path = os.path.join(dirname, '../storage/models')
    models = os.listdir(models_path)
    for idx, model in enumerate(models):
        model_path = os.path.join(models_path, model)
        with open(os.path.join(model_path, 'metadata.json'), 'r') as fp:
            model_metadata = json.load(fp)

        print(f'MODEL {idx} - {model}')
        print(model_metadata)

    which_model = input("Please, select a model (number) to explain or Enter to use the one in deployment: ")
    return which_model


def execute_model_validation():
    dirname = os.path.dirname(__file__)
    # load data
    con = duckdb.connect(database=os.path.join(dirname, '../../feature_generation/storage/datasets.duckdb'),read_only=False)
    test_features = con.execute("SELECT * FROM test_features").df()
    test_labels = con.execute("SELECT * FROM test_labels").df()
    test_labels = test_labels["medals"]
    con.close()
    # select model
    which_model = select_model_to_validate()
    models_path = os.path.join(dirname, '../storage/models')
    models = os.listdir(models_path)
    if which_model:
        model_path = os.path.join(models_path, models[int(which_model)])
    else:
        model_path = os.path.join(dirname, '../../deployment/model')

    # load the model
    model_file = os.path.join(model_path, "model.sav")
    model = pickle.load(open(model_file, 'rb'))

    # get metrics
    predictions = model.predict(test_features).astype(int)
    model_metrics = dict()
    model_metrics["MAE"] = metrics.mean_absolute_error(test_labels, predictions)
    model_metrics["MSE"] = metrics.mean_squared_error(test_labels, predictions, squared=False)
    model_metrics["RMSE"] = metrics.mean_squared_error(test_labels, predictions)
    model_metrics["MAPE"] = metrics.mean_absolute_percentage_error(test_labels, predictions)

    # get model_importance
    model_feature_importances = {test_features.columns[i]: model.feature_importances_[i] for i in
                                 range(len(test_features.columns))}

    # save model validation results
    with open(os.path.join(model_path, 'metadata.json'), 'r') as fp:
        model_metadata = json.load(fp)
        model_metadata["metrics"] = model_metrics
        model_metadata["feature_importance"] = model_feature_importances

    with open(os.path.join(model_path, 'metadata.json'), 'w') as fp:
        json.dump(model_metadata, fp)