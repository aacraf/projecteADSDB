# -*- coding: utf-8 -*-
"""featureGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aJ_PIstxFQS4ZR_O-x2fQ3ckqrR_SnST
"""
import os.path

import duckdb
import pandas as pd
from sklearn.model_selection import train_test_split


"""
The purpose of this notebook is to generate the different datasets that will be trained and will be tested. Basically here's where the data that will feed the models and the interpreters will be created.

In this case we only have an analytical case so we only generate a pair of train and test datasets. If there would be more analytical sandboxes they should be generated as well.

In this section is where "feature selection" should be applied. However, since our goal is to find which featrues are more important, all features will be included.

"""

## Load Data - Influential Indicators on Olympics



"""We can see that we have all indicators and also we have the number of medals obtained that year.

## Feature Generation

All features wil be included becouse the analytical goal is to obtain the feature importance for each variable. Let's drop all contextual information to only get the indicators
"""





"""##Train and test split

Now that we have featrues and labels identified and separatated, we will divide the data in a train and test split.

The train split will be a 75% of the total data and the train a 25%
"""



"""We can see the different dimensions for the splits

## Save Data

Finally, we save the splits as a new data
"""



def execute_feature_generation():
  dirname = os.path.dirname(__file__)
  
  # load data
  con = duckdb.connect(database=os.path.join(dirname, "../../analyticsandbox/storage/sandboxes/olympics_indicators.duckdb"), read_only=False)
  df = con.execute('SELECT * FROM Olympic_Indicators').df()

  # feature generation
  features = df.drop(['year', 'ioc_code', 'medals', 'city', 'iso', 'country', 'region',
       'olympic_type', 'notes'], axis=1)
  
  labels = df["medals"]

  # train and test split
  train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)
  
  # save as tables
  con2 = duckdb.connect(database=os.path.join(dirname, '../storage/datasets.duckdb'), read_only=False)
  ## needed transformation in order to save data
  train_labels_df = train_labels.to_frame().reset_index(drop=True)
  test_labels_df = test_labels.to_frame().reset_index(drop=True)

  ## Train tables
  con2.execute(f'DROP TABLE IF EXISTS train_features;')
  con2.execute(f'CREATE TABLE IF NOT EXISTS train_features AS SELECT * FROM train_features;')

  con2.execute(f'DROP TABLE IF EXISTS train_labels;')
  con2.execute(f'CREATE TABLE IF NOT EXISTS train_labels AS SELECT * FROM train_labels_df;')

  ## Test Tables
  con2.execute(f'DROP TABLE IF EXISTS test_features;')
  con2.execute(f'CREATE TABLE IF NOT EXISTS test_features AS SELECT * FROM test_features;')

  con2.execute(f'DROP TABLE IF EXISTS test_labels;')
  con2.execute(f'CREATE TABLE IF NOT EXISTS test_labels AS SELECT * FROM test_labels_df;')

  # close connections
  con.close()
  con2.close()